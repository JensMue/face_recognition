{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we perform the benchmarking of our emotion detection models. In particular, we will track two metrics:\n",
    "* weighted average precision\n",
    "* average prediction time\n",
    "\n",
    "In order to perform the benchmarking we selected 1000 random images from the test set, feeded them into the various models and computed the two metrics above. \n",
    "\n",
    "We have decided to track weighted average precision for benchmarking our models against each other. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. It is the fraction of events where the classifier correctly declared class i out of all instances where the classifier declared class i. Since our benchmarking dataset is imbalanced, we calculate the precision for each label and then find their average weighted by support, which is the number of true instances for each label. For balanced datasets, the weighted average precision is equal to average precision.\n",
    "\n",
    "Depending on the final implementation of our application, the prediction time of the model is also of relevance. Therefore, we test how fast each model outputs a prediction on average (in seconds). We are aware, that depending on the available computing power, prediction time may vary significantly. Yet, we are interested in the relative performance of our implemented models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random, os\n",
    "import time\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, average_precision_score, balanced_accuracy_score, classification_report\n",
    "\n",
    "# set image target Size\n",
    "IMG_HEIGHT = 96\n",
    "IMG_WIDTH = 96 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths relative to facial_expression_full folder: \n",
    "\n",
    "#only modify the facial_exp_dir variable such that the path reaches the folder facial_expression_full\n",
    "facial_exp_dir = \"/Users/Felix/datasets/facial_expression_full/\"\n",
    "\n",
    "#There is no need to modify the dependencies below. \n",
    "\n",
    "#Set Paths to models and weights: \n",
    "\n",
    "##\n",
    "#Models with architectures and weights in two different files: \n",
    "##\n",
    "# facial_expression_full/models_and_weights/sep_models/\"\n",
    "model_dir = facial_exp_dir + \"models_and_weights/sep_models/\"\n",
    "\n",
    "##\n",
    "#Models with architectures and weights in the same file: \n",
    "##\n",
    "# facial_expression_full/Models_and_Weights/full_models/\"\n",
    "models_p_directory = facial_exp_dir + \"models_and_weights/full_models/\"\n",
    "\n",
    "##\n",
    "#Set path to benchmarking directory\n",
    "##\n",
    "# facial_expression_full/benchmarking/\"\n",
    "benchmark_dir = facial_exp_dir + \"benchmarking/\"\n",
    "\n",
    "##\n",
    "#Set the directory for test images for benchmarking: \n",
    "##\n",
    "# facial_expression_full/benchmarking/test_images/\"\n",
    "input_dir = facial_exp_dir + \"benchmarking/test_images/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crying_African_244_0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fer0017590.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fer0018106.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fer0015716.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>astound_soldier_794_5.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  image_name  label\n",
       "0   crying_African_244_0.png      4\n",
       "1             fer0017590.png      3\n",
       "2             fer0018106.png      1\n",
       "3             fer0015716.png      2\n",
       "4  astound_soldier_794_5.png      2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the small test df \n",
    "small_benchmak_df = pd.read_csv(benchmark_dir +'small_benchmak_df')\n",
    "small_benchmak_df = small_benchmak_df[['image_name', 'label']]\n",
    "small_benchmak_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionaty mapping the image name with the numerically labeled emotion\n",
    "name_to_label_dict = pd.Series(small_benchmak_df.label.values,index=small_benchmak_df.image_name).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to decode and resize images\n",
    "def decode_and_resize(image_name):\n",
    "    image = tf.io.read_file(input_dir + image_name)\n",
    "    image = tf.image.decode_image(image, channels=3, dtype=\"uint8\")\n",
    "    image = tf.image.per_image_standardization(image) \n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    return tf.cast(image, tf.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for managing the models having architectures and weights in two different files\n",
    "class FacialExpressionModel(object):\n",
    "    \n",
    "    EXPRESSIONS_LIST_NUMERIC = [i for i in range(1,8)]\n",
    "    \n",
    "    def __init__(self, model_json_file, model_weights_file):\n",
    "        \n",
    "         # initite an empty list where to store the predictions\n",
    "        self.predictions = []\n",
    "        self.prediction_times = []\n",
    "        self.average_time=0\n",
    "        self.weighted_average_precision=0\n",
    "        \n",
    "        # load model from JSON file\n",
    "        with open(model_json_file, \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            self.loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        # load weights into the new model\n",
    "        self.loaded_model.load_weights(model_weights_file)\n",
    "        \n",
    "    def predict_emotion(self, img):\n",
    "        # get starting time\n",
    "        start_time = time.time()\n",
    "        # obtain the probability of each emotion\n",
    "        self.preds_probability = self.loaded_model.predict(img)\n",
    "        # get finishing time\n",
    "        finish_time = time.time()\n",
    "        # compute the prediction time\n",
    "        pred_time = finish_time - start_time\n",
    "        #append the prediction time\n",
    "        self.prediction_times.append(pred_time)\n",
    "        # obtain the label associated with the highest probability\n",
    "        pred = self.EXPRESSIONS_LIST_NUMERIC[np.argmax(self.preds_probability)]\n",
    "        # append the current prediction to the prediction list\n",
    "        self.predictions.append(pred)\n",
    "        \n",
    "    def compute_weighted_average_precision(self, y_true, y_pred):\n",
    "        weighted_average_precision= classification_report(y_true, y_pred, output_dict=True)['weighted avg']['precision']\n",
    "        self.weighted_average_precision  = weighted_average_precision\n",
    "        return weighted_average_precision\n",
    "        \n",
    "    def compute_avg_prediction_time(self):\n",
    "        avg_prediction_time = sum(self.prediction_times)/ len(self.prediction_times)\n",
    "        self.average_time = avg_prediction_time\n",
    "        return avg_prediction_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for managing the models having architectures and weights in the same files\n",
    "class FER_Models(object):\n",
    "    \n",
    "    EXPRESSIONS_LIST_NUMERIC = [i for i in range(1,8)]\n",
    "    \n",
    "    def __init__(self, model_weights_file):\n",
    "        \n",
    "         # initite an empty list where to store the predictions\n",
    "        self.predictions = []\n",
    "        self.prediction_times = []\n",
    "        self.average_time=0\n",
    "        self.weighted_average_precision=0\n",
    "\n",
    "        # load weights into the new model\n",
    "        self.loaded_model = load_model(model_weights_file)\n",
    "        \n",
    "    def predict_emotion(self, img):\n",
    "        # get starting time\n",
    "        start_time = time.time()\n",
    "        # obtain the probability of each emotion\n",
    "        self.preds_probability = self.loaded_model.predict(img)\n",
    "        # get finishing time\n",
    "        finish_time = time.time()\n",
    "        # compute the prediction time\n",
    "        pred_time = finish_time - start_time\n",
    "        #append the prediction time\n",
    "        self.prediction_times.append(pred_time)\n",
    "        # obtain the label associated with the highest probability\n",
    "        pred = self.EXPRESSIONS_LIST_NUMERIC[np.argmax(self.preds_probability)]\n",
    "        # append the current prediction to the prediction list\n",
    "        self.predictions.append(pred)\n",
    "        \n",
    "        #return FacialExpressionModel.EXPRESSIONS_LIST[np.argmax(self.preds)], self.preds_probability\n",
    "        \n",
    "    def compute_weighted_average_precision(self, y_true, y_pred):\n",
    "        weighted_average_precision= classification_report(y_true, y_pred, output_dict=True)['weighted avg']['precision']\n",
    "        self.weighted_average_precision  = weighted_average_precision\n",
    "        return weighted_average_precision\n",
    "        \n",
    "    def compute_avg_prediction_time(self):\n",
    "        avg_prediction_time = sum(self.prediction_times)/ len(self.prediction_times)\n",
    "        self.average_time = avg_prediction_time\n",
    "        return avg_prediction_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark of the models loaded from json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnet152V2.h5',\n",
       " 'resnet152V2_model.json',\n",
       " 'resnet_50_75e.h5',\n",
       " 'resnet_50_model.json',\n",
       " 'xception_35evaluation.h5',\n",
       " 'xception_model.json',\n",
       " 'xception_tl_ft.h5',\n",
       " 'xception_tl_ft_model.json',\n",
       " 'xception_tl_model.json',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain a list of all the models and the associated weights\n",
    "os.listdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary in which to store the relavent metrics\n",
    "# the structure will be: performance_dict[\"model_name\"] = [weighted_average_precision, average_time]\n",
    "performance_dict = dict()\n",
    "\n",
    "# create a list where to store the model_names and the model weights\n",
    "models = [\n",
    "    ['resnet_50_model.json', 'resnet_50_75e.h5'],\n",
    "    ['resnet152V2_model.json','resnet152V2.h5'],\n",
    "    ['xception_model.json','xception_35evaluation.h5'],\n",
    "    ['xception_tl_ft_model.json','xception_tl_ft.h5']  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading resnet_50_model\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of resnet_50_model\n",
      "computing the average prediction time of resnet_50_model\n",
      "\n",
      "loading resnet_50_tl_model\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of resnet_50_tl_model\n",
      "computing the average prediction time of resnet_50_tl_model\n",
      "\n",
      "loading resnet152V2_model\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of resnet152V2_model\n",
      "computing the average prediction time of resnet152V2_model\n",
      "\n",
      "loading xception_model\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of xception_model\n",
      "computing the average prediction time of xception_model\n",
      "\n",
      "loading xception_tl_ft_model\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of xception_tl_ft_model\n",
      "computing the average prediction time of xception_tl_ft_model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate over the models\n",
    "for model in models:\n",
    "    # load the model structure and its weights\n",
    "    print('loading {}'.format(model[0].split(\".\")[0]))\n",
    "    current_model = FacialExpressionModel(model_dir + model[0], model_dir + model[1])\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    print('getting the predictions')\n",
    "    # get the prediction for all 1000 testing images\n",
    "    for k,v in name_to_label_dict.items():\n",
    "        # load the image\n",
    "        img_deco = decode_and_resize(k)\n",
    "\n",
    "        # reshape as a fake batch \n",
    "        img_input = tf.expand_dims(img_deco, axis=0)\n",
    "\n",
    "        # get the prediction\n",
    "        current_model.predict_emotion(img_input)\n",
    "    print('predictions done')\n",
    "        \n",
    "    # compute the weighted_average_precision for the current model\n",
    "    print(\"computing the weighted average precision of {}\".format(model[0].split(\".\")[0]))\n",
    "    current_model.compute_weighted_average_precision(list(name_to_label_dict.values()), current_model.predictions)\n",
    "\n",
    "    # compute the average time it takes to make the prediction\n",
    "    print(\"computing the average prediction time of {}\".format(model[0].split(\".\")[0]))\n",
    "    current_model.compute_avg_prediction_time()\n",
    "    \n",
    "    # add the previous score to the performance_dict\n",
    "    performance_dict[model[0].split(\".\")[0]] = [current_model.weighted_average_precision, current_model.average_time]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resnet_50_model': [0.599479527115729, 0.27765147948265073],\n",
       " 'resnet152V2_model': [0.5734880351966888, 0.41249992537498475],\n",
       " 'xception_model': [0.597263020443714, 0.30021450996398924],\n",
       " 'xception_tl_ft_model': [0.5151578707494016, 0.32886655640602114]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the performance dictonary \n",
    "performance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark of the remaining models loaded from .h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inception_model_1_FD.h5',\n",
       " 'naive_inception_model_1_FD.h5',\n",
       " 'VGG16_model_FD.h5',\n",
       " 'VGG_Inception_1_FD.h5',\n",
       " 'VGG_model_1_FD.h5',\n",
       " 'VGG_model_2_FD.h5',\n",
       " 'ResNet50_TL_FT.hdf5']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of the models having both the architecture and the weights in the same file\n",
    "models_p = []\n",
    "for file in os.listdir(models_p_directory):\n",
    "    extension = file.split(\".\")[-1]\n",
    "    if extension == \"h5\" or extension == \"hdf5\":\n",
    "        models_p.append(file)\n",
    "models_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above list we will remove the naive_inception_model_1_FD and the VGG16_model_FD because they performed rather poorly during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inception_model_1_FD.h5',\n",
       " 'VGG_Inception_1_FD.h5',\n",
       " 'VGG_model_1_FD.h5',\n",
       " 'VGG_model_2_FD.h5',\n",
       " 'ResNet50_TL_FT.hdf5']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_p.remove(\"naive_inception_model_1_FD.h5\")\n",
    "models_p.remove(\"VGG16_model_FD.h5\")\n",
    "models_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading inception_model_1_FD.h5\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of inception_model_1_FD.h5\n",
      "computing the average prediction time of inception_model_1_FD.h5\n",
      "\n",
      "loading VGG_Inception_1_FD.h5\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of VGG_Inception_1_FD.h5\n",
      "computing the average prediction time of VGG_Inception_1_FD.h5\n",
      "\n",
      "loading VGG_model_1_FD.h5\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of VGG_model_1_FD.h5\n",
      "computing the average prediction time of VGG_model_1_FD.h5\n",
      "\n",
      "loading VGG_model_2_FD.h5\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of VGG_model_2_FD.h5\n",
      "computing the average prediction time of VGG_model_2_FD.h5\n",
      "\n",
      "loading ResNet50_TL_FT.hdf5\n",
      "model loaded\n",
      "getting the predictions\n",
      "predictions done\n",
      "computing the weighted average precision of ResNet50_TL_FT.hdf5\n",
      "computing the average prediction time of ResNet50_TL_FT.hdf5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate over the models\n",
    "performance_dict_p = dict()\n",
    "\n",
    "for model in models_p:\n",
    "    # load the model structure and its weights\n",
    "    print('loading {}'.format(model))\n",
    "    current_model = FER_Models(models_p_directory + model)\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    print('getting the predictions')\n",
    "    # get the prediction for all 1000 testing images\n",
    "    for k,v in name_to_label_dict.items():\n",
    "        # load the image\n",
    "        img_deco = decode_and_resize(k)\n",
    "\n",
    "        #reshape as a fake batch \n",
    "        img_input = tf.expand_dims(img_deco, axis=0)\n",
    "\n",
    "        # get the prediction\n",
    "        current_model.predict_emotion(img_input)\n",
    "    print('predictions done')\n",
    "        \n",
    "    # compute the weighted_average_precision for the current model\n",
    "    print(\"computing the weighted average precision of {}\".format(model))\n",
    "    current_model.compute_weighted_average_precision(list(name_to_label_dict.values()), current_model.predictions)\n",
    "\n",
    "    # compute the average time it takes to make the prediction\n",
    "    print(\"computing the average prediction time of {}\".format(model))\n",
    "    current_model.compute_avg_prediction_time()\n",
    "    \n",
    "    # add the previous score to the performance_dict\n",
    "    performance_dict_p[model] = [current_model.weighted_average_precision, current_model.average_time]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inception_model_1_FD.h5': [0.4562538960363645, 0.24310603976249695],\n",
       " 'VGG_Inception_1_FD.h5': [0.5249140183397178, 0.22430579900741576],\n",
       " 'VGG_model_1_FD.h5': [0.4978514525761788, 0.22617317914962767],\n",
       " 'VGG_model_2_FD.h5': [0.5429461955085444, 0.2309540946483612],\n",
       " 'ResNet50_TL_FT.hdf5': [0.5645912324122788, 0.28026635384559634]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the performance dictonary\n",
    "performance_dict_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resnet_50_model': [0.599479527115729, 0.27765147948265073],\n",
       " 'resnet152V2_model': [0.5734880351966888, 0.41249992537498475],\n",
       " 'xception_model': [0.597263020443714, 0.30021450996398924],\n",
       " 'xception_tl_ft_model': [0.5151578707494016, 0.32886655640602114],\n",
       " 'inception_model_1_FD.h5': [0.4562538960363645, 0.24310603976249695],\n",
       " 'VGG_Inception_1_FD.h5': [0.5249140183397178, 0.22430579900741576],\n",
       " 'VGG_model_1_FD.h5': [0.4978514525761788, 0.22617317914962767],\n",
       " 'VGG_model_2_FD.h5': [0.5429461955085444, 0.2309540946483612],\n",
       " 'ResNet50_TL_FT.hdf5': [0.5645912324122788, 0.28026635384559634]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the 2 performance dictionaries \n",
    "performance_dict_combined = {**performance_dict, **performance_dict_p}\n",
    "performance_dict_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the model names\n",
    "performance_dict_combined_clean = {'resnet50': [0.599479527115729, 0.27765147948265073],\n",
    "                                   'resnet152V2': [0.5734880351966888, 0.41249992537498475],\n",
    "                                   'xception': [0.597263020443714, 0.30021450996398924],\n",
    "                                   'xception tl': [0.5151578707494016, 0.32886655640602114],\n",
    "                                   'inception': [0.4562538960363645, 0.24310603976249695],\n",
    "                                   'VGG Inception': [0.5249140183397178, 0.22430579900741576],\n",
    "                                   'VGG v1': [0.4978514525761788, 0.22617317914962767],\n",
    "                                   'VGG v2': [0.5429461955085444, 0.2309540946483612],\n",
    "                                   'ResNet50 TL': [0.5645912324122788, 0.28026635384559634]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-8e7e8b63c0a0>:14: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(keys)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "keys = [key for key in performance_dict_combined_clean.keys()]\n",
    "values = [value for value in performance_dict_combined_clean.values()]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(np.arange(len(keys)) - 0.2, [value[0] for value in values],\n",
    "       width=0.2, color='b', align='center')\n",
    "\n",
    "ax.bar(np.arange(len(keys)) + 0.2,\n",
    "       [value[1] if len(value) == 2 else 0 for value in values],\n",
    "       width=0.2, color='g', align='center')\n",
    "\n",
    "ax.set_xticklabels(keys)\n",
    "ax.set_xticks(np.arange(len(keys)))\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "colors = {'weighted average precision':'blue', 'average time':'green'}  \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "plt.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the barplot above, ResNet50 is the model that achieves the highes weighted average precision while still keeping the average prediction time under 0.3 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weighted average precision</th>\n",
       "      <th>average time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>resnet50</th>\n",
       "      <td>0.599480</td>\n",
       "      <td>0.277651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet152V2</th>\n",
       "      <td>0.573488</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xception</th>\n",
       "      <td>0.597263</td>\n",
       "      <td>0.300215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xception tl</th>\n",
       "      <td>0.515158</td>\n",
       "      <td>0.328867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inception</th>\n",
       "      <td>0.456254</td>\n",
       "      <td>0.243106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VGG Inception</th>\n",
       "      <td>0.524914</td>\n",
       "      <td>0.224306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VGG v1</th>\n",
       "      <td>0.497851</td>\n",
       "      <td>0.226173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VGG v2</th>\n",
       "      <td>0.542946</td>\n",
       "      <td>0.230954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ResNet50 TL</th>\n",
       "      <td>0.564591</td>\n",
       "      <td>0.280266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               weighted average precision  average time\n",
       "resnet50                         0.599480      0.277651\n",
       "resnet152V2                      0.573488      0.412500\n",
       "xception                         0.597263      0.300215\n",
       "xception tl                      0.515158      0.328867\n",
       "inception                        0.456254      0.243106\n",
       "VGG Inception                    0.524914      0.224306\n",
       "VGG v1                           0.497851      0.226173\n",
       "VGG v2                           0.542946      0.230954\n",
       "ResNet50 TL                      0.564591      0.280266"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe and export the data\n",
    "performance_df=pd.DataFrame.from_dict(performance_dict_combined_clean,orient='index')\n",
    "# add the columsn names\n",
    "performance_df.columns = [\"weighted average precision\", \"average time\"]\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv file\n",
    "performance_df.to_csv(benchmark_dir + 'models_performance.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

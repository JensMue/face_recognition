{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2cSu17pDYcL"
   },
   "source": [
    "# Face Detector Training\n",
    "\n",
    "The training is done on the [Retinaface official annotations](https://drive.google.com/file/d/1vgCABX1JI3NGBzsHxwBXlmRjaLV3NIsG/view) for the [WIDER FACE dataset](http://shuoyang1213.me/WIDERFACE/index.html); a set containing thousands of different images taken from different contexts (e.g. marching band at a parade, people during demonstrations/riots, people inside shopping malls and so on). The official annotations differ from the original ones (consisting of only the bounding box ground truth), since RetinaFace is trained to do Classification, BoundingBox regression and Landmark regression, example:\n",
    ">\\# 0--Parade/0_Parade_marchingband_1_849.jpg\n",
    ">\n",
    "> \\# [subdir/image_name]\n",
    ">\n",
    ">449 330 122 149  488.906 373.643 0.0  542.089 376.442 0.0  515.031 412.83 0.0  485.174 425.893 0.0  538.357 431.491 0.0  0.82\n",
    ">\n",
    ">[bounding box]  [left eye]  [right eye]  [nose]  [left corner mouth]  [right corner mouth]  [validity]\n",
    "\n",
    "The folder structure for converting the dataset and then training should be like this:\n",
    "\n",
    "`\n",
    "face_detection/data/widerface/train/\n",
    "                               images/\n",
    "                               label.txt`\n",
    "        \n",
    "The structure of this notebook is as follows (internal hyperlinks do not work on colab):\n",
    " - [Converting the dataset to tensorflow record for training](#convert)\n",
    " - [Training the model](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0N180SYDYcM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cirib\\Desktop\\MainFolder\n"
     ]
    }
   ],
   "source": [
    "# due to relative imports from modules, this notebook should be run from the main folder hence the 'cd'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mKebdrw6DYcM"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from absl import logging\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from face_detection.models import RetinaFaceModel\n",
    "from face_detection.lr_scheduler import MultiStepWarmUpLR\n",
    "from face_detection.losses import MultiBoxLoss\n",
    "from face_detection.anchor import prior_box\n",
    "from face_detection.utils import (set_memory_growth, load_yaml, load_dataset, ProgressBar)\n",
    "\n",
    "\n",
    "\n",
    "set_memory_growth()# avoid memory problems\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  \n",
    "\n",
    "# get rid of tensorflow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb9fwVeyDYcM"
   },
   "source": [
    "<a id ='convert'></a>\n",
    "## Converting the dataset\n",
    "\n",
    "From the WIDER FACE dataset, the training set contains 12880 images with ground truth for face bounding boxes, facial landmarks and validity of face (e.g. '-1.0' faces do not have landmarks because they are too blurred or otherwise undetectable).\n",
    "\n",
    "The .zip doesn't contain the tfrecord used for training, the next two cells will easily create it.\n",
    "\n",
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XDVOk3oPDYcM"
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def make_example(img_name, img_path, target):\n",
    "    # Create a dictionary with features.\n",
    "    feature = {'image/img_name': _bytes_feature([img_name]),\n",
    "               'image/object/bbox/xmin': _float_feature(target[:, 0]),\n",
    "               'image/object/bbox/ymin': _float_feature(target[:, 1]),\n",
    "               'image/object/bbox/xmax': _float_feature(target[:, 2]),\n",
    "               'image/object/bbox/ymax': _float_feature(target[:, 3]),\n",
    "               'image/object/landmark0/x': _float_feature(target[:, 4]),\n",
    "               'image/object/landmark0/y': _float_feature(target[:, 5]),\n",
    "               'image/object/landmark1/x': _float_feature(target[:, 6]),\n",
    "               'image/object/landmark1/y': _float_feature(target[:, 7]),\n",
    "               'image/object/landmark2/x': _float_feature(target[:, 8]),\n",
    "               'image/object/landmark2/y': _float_feature(target[:, 9]),\n",
    "               'image/object/landmark3/x': _float_feature(target[:, 10]),\n",
    "               'image/object/landmark3/y': _float_feature(target[:, 11]),\n",
    "               'image/object/landmark4/x': _float_feature(target[:, 12]),\n",
    "               'image/object/landmark4/y': _float_feature(target[:, 13]),\n",
    "               'image/object/landmark/valid': _float_feature(target[:, 14])}\n",
    "    img_str = open(img_path, 'rb').read()\n",
    "    feature['image/encoded'] = _bytes_feature([img_str])\n",
    "    \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def load_info(txt_path):\n",
    "    \"\"\"reads the annotations from the label.txt\"\"\"\n",
    "    img_paths = []\n",
    "    words = []\n",
    "\n",
    "    f = open(txt_path, 'r')\n",
    "    lines = f.readlines()\n",
    "    isFirst = True\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        line = line.rstrip()\n",
    "        if line.startswith('#'): # path to image indicator\n",
    "            if isFirst is True:\n",
    "                isFirst = False\n",
    "            else:\n",
    "                labels_copy = labels.copy()\n",
    "                words.append(labels_copy)\n",
    "                labels.clear()\n",
    "            path = line[2:]\n",
    "            path = txt_path.replace('label.txt', 'images/') + path\n",
    "            img_paths.append(path)\n",
    "        else:\n",
    "            line = line.split(' ')\n",
    "            label = [float(x) for x in line]\n",
    "            labels.append(label)\n",
    "\n",
    "    words.append(labels)\n",
    "    return img_paths, words\n",
    "\n",
    "\n",
    "def get_target(labels):\n",
    "    annotations = np.zeros((0, 15))\n",
    "    if len(labels) == 0:\n",
    "        return annotations\n",
    "    for idx, label in enumerate(labels):\n",
    "        annotation = np.zeros((1, 15))\n",
    "        # bbox\n",
    "        annotation[0, 0] = label[0]  # x1\n",
    "        annotation[0, 1] = label[1]  # y1\n",
    "        annotation[0, 2] = label[0] + label[2]  # x2\n",
    "        annotation[0, 3] = label[1] + label[3]  # y2\n",
    "\n",
    "        # landmarks\n",
    "        annotation[0, 4] = label[4]    # l0_x\n",
    "        annotation[0, 5] = label[5]    # l0_y\n",
    "        annotation[0, 6] = label[7]    # l1_x\n",
    "        annotation[0, 7] = label[8]    # l1_y\n",
    "        annotation[0, 8] = label[10]   # l2_x\n",
    "        annotation[0, 9] = label[11]   # l2_y\n",
    "        annotation[0, 10] = label[13]  # l3_x\n",
    "        annotation[0, 11] = label[14]  # l3_y\n",
    "        annotation[0, 12] = label[16]  # l4_x\n",
    "        annotation[0, 13] = label[17]  # l4_y\n",
    "        if (annotation[0, 4] < 0):\n",
    "            annotation[0, 14] = -1  # w/o landmark\n",
    "        else:\n",
    "            annotation[0, 14] = 1\n",
    "\n",
    "        annotations = np.append(annotations, annotation, axis=0)\n",
    "    target = np.array(annotations)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5g9YqDeDYcM",
    "outputId": "4516f599-3ecf-432d-8c13-9804e8369713"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 12880/12880 [00:38<00:00, 338.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# set dataset path\n",
    "dataset_path = 'face_detection/data/widerface/train'\n",
    "\n",
    "# reads info\n",
    "img_paths, words = load_info(os.path.join(dataset_path, 'label.txt'))\n",
    "samples = list(zip(img_paths, words))\n",
    "random.shuffle(samples)\n",
    "\n",
    "if os.path.exists('face_detection/data/widerface_train_bin.tfrecord'):\n",
    "    raise Exception(\"tfrecord already exists\")\n",
    "\n",
    "# writing tfredcord\n",
    "with tf.io.TFRecordWriter('face_detection/data/widerface_train_bin.tfrecord') as writer:\n",
    "    for img_path, word in tqdm.tqdm(samples):\n",
    "        target = get_target(word)\n",
    "        img_name = os.path.basename(img_path).replace('.jpg', '')\n",
    "\n",
    "        tf_example = make_example(img_name=str.encode(img_name),\n",
    "                                  img_path=str.encode(img_path),\n",
    "                                  target=target)\n",
    "\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCwh9xL7DYcN"
   },
   "source": [
    "<a id ='train'></a>\n",
    "## Training\n",
    "\n",
    "<p style=\"color:#FF0000\";> Do not run if you're on cpu</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4OXkR95DYcN",
    "outputId": "97171465-aeb8-4a92-f610-69cd8532a1bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 2s 0us/step\n",
      "Model: \"RetinaFaceModel\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)              Output Shape      Param #  Connected to               \n",
      "================================================================================\n",
      "input_image (InputLayer)  [(None, 640, 640, 0                                   \n",
      "________________________________________________________________________________\n",
      "tf_op_layer_strided_slice [(None, 640, 640, 0        input_image[0][0]          \n",
      "________________________________________________________________________________\n",
      "tf_op_layer_BiasAdd (Tens [(None, 640, 640, 0        tf_op_layer_strided_slice[0\n",
      "________________________________________________________________________________\n",
      "ResNet50_extractor (Funct ((None, 80, 80, 5 23587712 tf_op_layer_BiasAdd[0][0]  \n",
      "________________________________________________________________________________\n",
      "FPN (FPN)                 ((None, 80, 80, 2 2102272  ResNet50_extractor[0][0]   \n",
      "                                                     ResNet50_extractor[0][1]   \n",
      "                                                     ResNet50_extractor[0][2]   \n",
      "________________________________________________________________________________\n",
      "SSH_0 (SSH)               (None, 80, 80, 25 554496   FPN[0][0]                  \n",
      "________________________________________________________________________________\n",
      "SSH_1 (SSH)               (None, 40, 40, 25 554496   FPN[0][1]                  \n",
      "________________________________________________________________________________\n",
      "SSH_2 (SSH)               (None, 20, 20, 25 554496   FPN[0][2]                  \n",
      "________________________________________________________________________________\n",
      "ClassHead_0 (ClassHead)   (None, None, 2)   1028     SSH_0[0][0]                \n",
      "________________________________________________________________________________\n",
      "ClassHead_1 (ClassHead)   (None, None, 2)   1028     SSH_1[0][0]                \n",
      "________________________________________________________________________________\n",
      "ClassHead_2 (ClassHead)   (None, None, 2)   1028     SSH_2[0][0]                \n",
      "________________________________________________________________________________\n",
      "BboxHead_0 (BboxHead)     (None, None, 4)   2056     SSH_0[0][0]                \n",
      "________________________________________________________________________________\n",
      "BboxHead_1 (BboxHead)     (None, None, 4)   2056     SSH_1[0][0]                \n",
      "________________________________________________________________________________\n",
      "BboxHead_2 (BboxHead)     (None, None, 4)   2056     SSH_2[0][0]                \n",
      "________________________________________________________________________________\n",
      "LandmarkHead_0 (LandmarkH (None, None, 10)  5140     SSH_0[0][0]                \n",
      "________________________________________________________________________________\n",
      "LandmarkHead_1 (LandmarkH (None, None, 10)  5140     SSH_1[0][0]                \n",
      "________________________________________________________________________________\n",
      "LandmarkHead_2 (LandmarkH (None, None, 10)  5140     SSH_2[0][0]                \n",
      "________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (Ten [(None, None, 2)] 0        ClassHead_0[0][0]          \n",
      "                                                     ClassHead_1[0][0]          \n",
      "                                                     ClassHead_2[0][0]          \n",
      "________________________________________________________________________________\n",
      "tf_op_layer_concat (Tenso [(None, None, 4)] 0        BboxHead_0[0][0]           \n",
      "                                                     BboxHead_1[0][0]           \n",
      "                                                     BboxHead_2[0][0]           \n",
      "________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (Ten [(None, None, 10) 0        LandmarkHead_0[0][0]       \n",
      "                                                     LandmarkHead_1[0][0]       \n",
      "                                                     LandmarkHead_2[0][0]       \n",
      "________________________________________________________________________________\n",
      "softmax (Softmax)         (None, None, 2)   0        tf_op_layer_concat_2[0][0] \n",
      "================================================================================\n",
      "Total params: 27,378,144\n",
      "Trainable params: 27,320,160\n",
      "Non-trainable params: 57,984\n",
      "________________________________________________________________________________\n",
      "[*] load ckpt from face_detection/checkpoints/retinaface_res50/ckpt-29 at step 58000.\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=37/100, loss=0.5243, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>                   ] 430/1610, epoch=38/100, loss=0.5835, lr=1.0e-02  0.5 step/sec\n",
      "[*] save ckpt file at face_detection/checkpoints/retinaface_res50/ckpt-30\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=38/100, loss=0.3104, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>>>>>>>             ] 820/1610, epoch=39/100, loss=0.3330, lr=1.0e-02  0.5 step/sec\n",
      "[*] save ckpt file at face_detection/checkpoints/retinaface_res50/ckpt-31\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=39/100, loss=0.3331, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>>>>>>>>>>>>>       ] 1210/1610, epoch=40/100, loss=0.3610, lr=1.0e-02  0.5 step/sec\n",
      "[*] save ckpt file at face_detection/checkpoints/retinaface_res50/ckpt-32\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=40/100, loss=0.2608, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>> ] 1600/1610, epoch=41/100, loss=0.6625, lr=1.0e-02  0.5 step/sec\n",
      "[*] save ckpt file at face_detection/checkpoints/retinaface_res50/ckpt-33\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=41/100, loss=0.5078, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=42/100, loss=0.3992, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>                    ] 380/1610, epoch=43/100, loss=0.1977, lr=1.0e-02  0.5 step/sec\n",
      "[*] save ckpt file at face_detection/checkpoints/retinaface_res50/ckpt-34\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=43/100, loss=0.4511, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>>>>>>              ] 770/1610, epoch=44/100, loss=0.2721, lr=1.0e-02  0.5 step/sec\n",
      "[*] save ckpt file at face_detection/checkpoints/retinaface_res50/ckpt-35\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=44/100, loss=0.3641, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>>>>>>>>>>>>>       ] 1160/1610, epoch=45/100, loss=0.4512, lr=1.0e-02  0.5 step/sec\n",
      "[*] save ckpt file at face_detection/checkpoints/retinaface_res50/ckpt-36\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=45/100, loss=0.6128, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>> ] 1550/1610, epoch=46/100, loss=0.2582, lr=1.0e-02  0.5 step/sec\n",
      "[*] save ckpt file at face_detection/checkpoints/retinaface_res50/ckpt-37\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>>>>>] 1610/1610, epoch=46/100, loss=0.3302, lr=1.0e-02  0.5 step/sec\n",
      "Training [>>>>>>>>>>>>>>>>>>>>>    ] 1367/1610, epoch=47/100, loss=0.7038, lr=1.0e-02  0.5 step/sec"
     ]
    }
   ],
   "source": [
    "# instance network\n",
    "cfg = load_yaml('face_detection/configs/retinaface_res50.yaml') # res50 or mbv2\n",
    "model = RetinaFaceModel(cfg, training=True)\n",
    "model.summary(line_length=80)\n",
    "\n",
    "# define prior box\n",
    "priors = prior_box((cfg['input_size'], cfg['input_size']),\n",
    "                   cfg['min_sizes'],  cfg['steps'], cfg['clip'])\n",
    "\n",
    "# load dataset\n",
    "train_dataset = load_dataset(cfg, priors, shuffle=True)\n",
    "\n",
    "# define optimizer\n",
    "steps_per_epoch = cfg['dataset_len'] // cfg['batch_size']\n",
    "learning_rate = MultiStepWarmUpLR(\n",
    "    initial_learning_rate=cfg['init_lr'],\n",
    "    lr_steps=[e * steps_per_epoch for e in cfg['lr_decay_epoch']],\n",
    "    lr_rate=cfg['lr_rate'],\n",
    "    warmup_steps=cfg['warmup_epoch'] * steps_per_epoch,\n",
    "    min_lr=cfg['min_lr'])\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "# define losses function\n",
    "multi_box_loss = MultiBoxLoss()\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint_dir = 'face_detection/checkpoints/' + cfg['sub_name']\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(0, name='step'),\n",
    "                                 optimizer=optimizer,\n",
    "                                 model=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
    "                                     directory=checkpoint_dir,\n",
    "                                     max_to_keep=3)\n",
    "if manager.latest_checkpoint:\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    print('[*] load ckpt from {} at step {}.'.format(manager.latest_checkpoint, checkpoint.step.numpy()))\n",
    "else:\n",
    "    print(\"[*] training from scratch.\")\n",
    "    \n",
    "# define training step function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "\n",
    "        losses = {}\n",
    "        losses['reg'] = tf.reduce_sum(model.losses)\n",
    "        losses['loc'], losses['landm'], losses['class'] = \\\n",
    "            multi_box_loss(labels, predictions)\n",
    "        total_loss = tf.add_n([l for l in losses.values()])\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return total_loss, losses\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer('face_detection/logs/' + cfg['sub_name'])       # tf log\n",
    "remain_steps = max(steps_per_epoch * cfg['epoch'] - checkpoint.step.numpy(), 0)   # remaining steps from last training\n",
    "prog_bar = ProgressBar(steps_per_epoch,checkpoint.step.numpy() % steps_per_epoch) # progress bar for visualization\n",
    "\n",
    "# training loop\n",
    "for inputs, labels in train_dataset.take(remain_steps):\n",
    "    checkpoint.step.assign_add(1)\n",
    "    steps = checkpoint.step.numpy()\n",
    "\n",
    "    total_loss, losses = train_step(inputs, labels)\n",
    "\n",
    "    prog_bar.update(\"epoch={}/{}, loss={:.4f}, lr={:.1e}\".format(\n",
    "        ((steps - 1) // steps_per_epoch) + 1, cfg['epoch'],\n",
    "        total_loss.numpy(), optimizer.lr(steps).numpy()))\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/total_loss', total_loss, step=steps)\n",
    "            for k, l in losses.items():\n",
    "                tf.summary.scalar('loss/{}'.format(k), l, step=steps)\n",
    "            tf.summary.scalar('learning_rate', optimizer.lr(steps), step=steps)\n",
    "\n",
    "    # make checkpoint\n",
    "    if steps % cfg['save_steps'] == 0:\n",
    "        manager.save()\n",
    "        print(\"\\n[*] save ckpt file at {}\".format(manager.latest_checkpoint))\n",
    "\n",
    "manager.save()\n",
    "print(\"\\n[*] training done! save ckpt file at {}\".format(manager.latest_checkpoint))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TADFuE7_M3Tv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FaceDetector_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
